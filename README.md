2020.07.31：
=====================
仅以此repository记录AI学习的过程，并熟悉对Github这一宅男交友网站的使用
当前学习主要在Kaggle平台上展开，记录学习过程中的感悟、进步等值得记录的地方。   

Kaggle介绍：
=====================
https://www.kaggle.com/
Kaggle成立于2010年，是一个进行数据发掘和预测竞赛的在线平台。目前网上关于Kaggle的介绍和教学已经非常多了，知乎、CSDN、简书等知识共享平台上都有，这里就不多做介绍。   
Kaggle一周能够免费提供30小时的算力时间，经过初略的估算（xjb估），其计算速度应当大于RTX2080,略等同于RTX2080TI。  对于硬件略显不足但是又想参加比赛（特别是CV方向深层网络的比赛）的同学来说是在是一项难得福音。

主要内容：
=====================
以具体的比赛为主，通过记录参与不同比赛的整个过程，写下感悟心得  
希望能够持之以恒

Global Wheat Detection
=====================
_Can you help identify wheat heads using image analysis?_

# 一、代码整体架构  
就目前我浅薄的经历来看，一个完整的深度学习code应当具有：数据模块、模型模块、结果模块这三个部分。
1.数据模块的主要功能是数据导入、数据切分、数据增强等，特别是其中的数据增强，对于结果有着较大的影响。
2.模型模块主要是模型导入、模型训练、优化器设置、丢失函数设置等，通常使用的都是已经组装好的库
3.结果模块较为简单，将Test数据导入到训练好的模型中，得到最终的结果并按照比赛的要求生成特定的文件，一般都是csv格式。
# 二、具体代码

# 三、方法技巧
### *Pseudo Label*

初次接触伪标签这一概念，目前并不能从原理上很好的理解之一方法，但是在今天FasterRCNN的实践当中，PL确实能够在一定程度上提升LB的成绩。

对于深度学习而言，拥有标准Label的数据是极其宝贵的，但是另一方面，由于DP的本质使得其对于图像数量的需求是极大的。因此，目前已经出现很多“奇奇怪怪”的方法来增加的数据集的数量。常见的方法有以翻转为基础的Image Augmentation、目前火热的迁移学习等。而Pseudo Label也是其中的一种方式。说实话，这种方式有点类似于遥感中常用的半监督分类。其具体步骤如下：

* 1.利用已有标准数据集训练模型

* 2.将无标签数据导入上面训练好的模型中获得无验证的标签数据

* 3.混合两种数据集再一次训练模型  

  经过上述步骤能够一定程度上提升模型精度  

  不过，目前我并不能很好的从原理上理解使用这一方法及其效果的原因。
 
### *论文阅读方法*
  不得不承认，单纯的使用现成方法（掉包侠）会在熟练度增长到一定程度后遇到瓶颈，因此便一边学习理论一边代码实践是不可或缺的。下面展示的吴恩达教授关于阅读论文的一点方法，特此记录，希望能为后面的论文阅读提供参考
  * title abstract figures  
  * introduction conclusion figures  and simple read  
  * whole theory and formal/funciton skip  
  * 在完成上述之后，应当能够描述论文作者的目标或者论文实现
  * 描述论文中方法的key  
  * 记录有用的内容  
  * 追踪有关线索、论文  
  
### *Augmentation*  

调用了Kaggle一个notebook中的图像增强方式，目前只是单纯的调用，并没有从源代码处理解其方法和原理，希望后续有时间来细读这一串code。

遇到的一个问题是提供的数据输出方式并不能直接导入到model当中，需要经过转化，目前想到的方式有三：

* 将得到的图片和标签先导出到本地，再通过已有的读取方式进行读取。这种方式思路简单但是实现繁琐，耗费内存。
* 修改目前的Loader，将转换后的图片模仿已有的loader构建新的sample，目前正在尝试当中
* 构建全新的loader，将上一步得到的新的图像作为新的数据集导入   

遇到问题的原因：在利用Pytorch进行深度学习的操作时，常常遇到的问题便是格式的转化，模型需要tensor格式数据，而我们在操作的时候往往使用的是numpy下array格式的数据，对于这一部分的理解还不是非常的深刻，转化是需要注意的点也没有非常的清楚，需要后续构建清晰的理解。
